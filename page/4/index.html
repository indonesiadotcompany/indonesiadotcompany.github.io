<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
	<meta name="generator" content="Hugo 0.57.2" />
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog Indonesiadot.Com &middot; Web log of Indonesiadot.Com, Linux and updated technology
    
  </title>

  
  <link rel="stylesheet" href="https://blog.indonesiadot.com/css/poole.css">
  <link rel="stylesheet" href="https://blog.indonesiadot.com/css/syntax.css">
  <link rel="stylesheet" href="https://blog.indonesiadot.com/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://blog.indonesiadot.com/assets/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="https://blog.indonesiadot.com/assets/favicon.ico">

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.indonesiadot.com/atom.xml">
</head>


  <body class="theme-base-0c">

    
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Blog dari <a href="https://indonesiadot.com" target="_blank">Indonesiadot.Com</a> adalah tempat menuliskan sesuatu mengenai teknologi, oprek, dan mengenai Indonesiadot.Com.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item  active " href="https://blog.indonesiadot.com/">Home</a>
    
    <a class="sidebar-nav-item" href="https://blog.indonesiadot.com/post">Posts</a>
    <a class="sidebar-nav-item" href="https://blog.indonesiadot.com/tags">Tags</a>

    
    
      
        <a class="sidebar-nav-item " href="https://blog.indonesiadot.com/about/">About</a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a class="sidebar-nav-item " href="https://blog.indonesiadot.com/thanks/">Thanks</a>
      
    

  </nav>

  <div class="sidebar-item">
    <p>&copy; 2021. All rights reserved.</p>
  </div>
</div>


    
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://blog.indonesiadot.com/" title="Home">Blog Indonesiadot.Com</a>
            <small>Web log of Indonesiadot.Com, Linux and updated technology</small>
          </h3>
        </div>
      </div>

      <div class="container content">





<div class="post">
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/06/12/postgresql/">Postgresql</a></h1>
        <span class="post-date">Jun 12, 2020</span>
	

<h2 id="install-pgadmin4-di-ubuntu-18">Install pgadmin4 di ubuntu 18</h2>

<pre><code>apt-get install curl ca-certificates gnupg
curl https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main&quot; &gt; /etc/apt/sources.list.d/pgdg.list'
apt update
apt install pgadmin4
</code></pre>

<h2 id="create-db">create db</h2>

<pre><code>su - postgres
psql
create database dvdrental;
\q
wget https://sp.postgresqltutorial.com/wp-content/uploads/2019/05/dvdrental.zip
unzip dvdrental.zip
pg_restore -d dvdrental dvdrental.tar
psql -d dvdrental
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO pgtower;
\q
psql -U pgtower -d dvdrental
select first_name from customer;
\q
</code></pre>

<h2 id="pg-hba-conf">pg_hba.conf</h2>

<pre><code>cat /var/opt/rh/rh-postgresql10/lib/pgsql/data/pg_hba.conf

# Database administrative login by UNIX sockets
# note: you may wish to restrict this further later
local   all         postgres                                peer

# TYPE  DATABASE    USER        CIDR-ADDRESS          METHOD
local   all         all                               scram-sha-256
host    all         all         127.0.0.1/32 scram-sha-256
host    all         all         ::1/128 scram-sha-256
host    all         all         10.11.8.0/24    md5
</code></pre>

<p>Referensi:</p>

<ul>
<li><a href="https://www.postgresqltutorial.com/postgresql-sample-database/">https://www.postgresqltutorial.com/postgresql-sample-database/</a></li>
<li><a href="https://bobcares.com/blog/permission-denied-for-database-postgres/">https://bobcares.com/blog/permission-denied-for-database-postgres/</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/beginner/">beginner</a>, <a href="https://blog.indonesiadot.com/tags/postgresql/">postgresql</a>, <a href="https://blog.indonesiadot.com/tags/psql/">psql</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/04/04/elk-elasticsearch-logstash-kibana-references/">Elk Elasticsearch Logstash Kibana References</a></h1>
        <span class="post-date">Apr 4, 2020</span>
	<ul>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-centos-7">https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-centos-7</a></li>
<li></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/elk/">elk</a>, <a href="https://blog.indonesiadot.com/tags/elasticsearch/">elasticsearch</a>, <a href="https://blog.indonesiadot.com/tags/logstash/">logstash</a>, <a href="https://blog.indonesiadot.com/tags/kibana/">kibana</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/04/03/ansible-ci-cd-cicd/">Ansible CI-CD CICD</a></h1>
        <span class="post-date">Apr 3, 2020</span>
	

<h2 id="jenkins-ansible-tower">Jenkins Ansible Tower</h2>

<p><img src="https://blog.indonesiadot.com/images/2020/continuous-delivery1.png" alt="ansible-cicd" /></p>

<ul>
<li><a href="https://plugins.jenkins.io/ansible-tower/">https://plugins.jenkins.io/ansible-tower/</a></li>
<li><a href="https://www.redhat.com/en/blog/take-ansible-and-jenkins-integration-next-level-cicd-ansible-tower">https://www.redhat.com/en/blog/take-ansible-and-jenkins-integration-next-level-cicd-ansible-tower</a></li>
<li><a href="https://www.redhat.com/en/blog/integrating-ansible-jenkins-cicd-process">https://www.redhat.com/en/blog/integrating-ansible-jenkins-cicd-process</a></li>
<li><a href="https://dzone.com/articles/how-to-setup-continuous-delivery-environment">https://dzone.com/articles/how-to-setup-continuous-delivery-environment</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/ansible/">ansible</a>, <a href="https://blog.indonesiadot.com/tags/cicd/">cicd</a>, <a href="https://blog.indonesiadot.com/tags/jenkins/">jenkins</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/04/03/security-compliance-baseline-benchmark/">Security Compliance Baseline Benchmark</a></h1>
        <span class="post-date">Apr 3, 2020</span>
	

<h2 id="ssg-baseline">ssg baseline</h2>

<ul>
<li><a href="https://static.open-scap.org/">https://static.open-scap.org/</a></li>
<li><a href="https://static.open-scap.org/ssg-guides/ssg-rhel7-guide-index.html">https://static.open-scap.org/ssg-guides/ssg-rhel7-guide-index.html</a></li>
</ul>

<h2 id="cis-baseline">cis baseline</h2>

<ul>
<li><a href="https://www.cisecurity.org/cis-benchmarks/">https://www.cisecurity.org/cis-benchmarks/</a></li>
<li><a href="https://indonesiadot.com">https://indonesiadot.com</a></li>
<li><a href="https://birau.com">https://birau.com</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/security/">security</a>, <a href="https://blog.indonesiadot.com/tags/baseline/">baseline</a>, <a href="https://blog.indonesiadot.com/tags/benchmark/">benchmark</a>, <a href="https://blog.indonesiadot.com/tags/pci-dss/">pci-dss</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/03/27/sentinelone-epp-edr-on-premise/">Sentinelone EPP EDR On-Premise</a></h1>
        <span class="post-date">Mar 27, 2020</span>
	

<h2 id="prerequisites-and-preparation">Prerequisites and preparation</h2>

<h3 id="hardware-requirements-management-versions-grand-canyon-and-later">Hardware Requirements: Management Versions Grand Canyon and later</h3>

<ul>
<li>Agents 10,000 to 20,000</li>
<li>CPU 16 Cores</li>
<li>RAM 32 GB</li>

<li><p>Disk Root: 16 GB, Disk 500 GB XFS</p></li>

<li><p><a href="https://support.sentinelone.com/hc/en-us/articles/360010427613-On-Prem-OVA-Prerequisites-and-Preparation">https://support.sentinelone.com/hc/en-us/articles/360010427613-On-Prem-OVA-Prerequisites-and-Preparation</a></p></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/epp/">epp</a>, <a href="https://blog.indonesiadot.com/tags/edr/">edr</a>, <a href="https://blog.indonesiadot.com/tags/sentinelone/">sentinelone</a>, <a href="https://blog.indonesiadot.com/tags/on-premise/">on-premise</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/03/20/certificate-ssl-key-csr-crt/">Certificate ssl key csr crt</a></h1>
        <span class="post-date">Mar 20, 2020</span>
	

<h2 id="create-csr-ubuntu-18">create csr ubuntu 18</h2>

<pre><code>openssl genrsa –out domain.key 2048
openssl req –new –key domain.key –out domain.csr
#or
#openssl req -new -newkey rsa:2048 -nodes -keyout domain.key -out domain.csr
</code></pre>

<ul>
<li><a href="https://vitux.com/how-to-generate-a-certificate-signing-request-csr-on-ubuntu/">https://vitux.com/how-to-generate-a-certificate-signing-request-csr-on-ubuntu/</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-self-signed-ssl-certificate-for-apache-in-ubuntu-18-04">https://www.digitalocean.com/community/tutorials/how-to-create-a-self-signed-ssl-certificate-for-apache-in-ubuntu-18-04</a></li>
</ul>

<h2 id="random-number-generator-rand-load-file-cannot-open-file-crypto-rand-randfile-c-88-filename-root-rnd">random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/root/.rnd</h2>

<p>If you have RANDFILE=&hellip; configuration lines in your openssl.cnf than you can safely remove them</p>

<ul>
<li><a href="https://github.com/openssl/openssl/issues/7754">https://github.com/openssl/openssl/issues/7754</a></li>
</ul>

<h2 id="convert-p7b-p7c-pkcs7-to-crt">convert p7b / p7c / pkcs7 to crt</h2>

<h3 id="description">Description</h3>

<p>What are the differences between .p7b, .pfx, .p12, .pem, .der, .crt &amp; .cer Certificates?</p>

<p>With so many servers, some require different formats.</p>

<h2 id="pem-format">PEM Format</h2>

<ul>
<li>It is the most common format used for certificates</li>
<li>Most servers (Ex: Apache) expects the certificates and private key to be in a separate files

<ul>
<li>Usually they are Base64 encoded ASCII files</li>
<li>Extensions used for PEM certificates are .cer, .crt, .pem, .key files</li>
<li>Apache and similar server uses PEM format certificates</li>
</ul></li>
</ul>

<h2 id="der-format">DER Format</h2>

<ul>
<li>The DER format is the binary form of the certificate</li>
<li>All types of certificates &amp; private keys can be encoded in DER format</li>
<li>DER formatted certificates do not contain the &ldquo;BEGIN CERTIFICATE/END CERTIFICATE&rdquo; statements</li>
<li>DER formatted certificates most often use the ‘.cer’ and &lsquo;.der&rsquo; extensions</li>
<li>DER is typically used in Java Platforms</li>
</ul>

<h2 id="p7b-pkcs-7-format">P7B/PKCS#7 Format</h2>

<ul>
<li>The PKCS#7 or P7B format is stored in Base64 ASCII format and has a file extension of .p7b or .p7c</li>
<li>A P7B file only contains certificates and chain certificates (Intermediate CAs), not the private key</li>
<li>The most common platforms that support P7B files are Microsoft Windows and Java Tomcat</li>
</ul>

<h2 id="pfx-p12-pkcs-12-format">PFX/P12/PKCS#12 Format</h2>

<ul>
<li>The PKCS#12 or PFX/P12 format is a binary format for storing the server certificate, intermediate certificates, and the private key in one encryptable file</li>
<li>These files usually have extensions such as .pfx and .p12</li>
<li>They are typically used on Windows machines to import and export certificates and private keys</li>
</ul>

<p>If your server/device requires a different certificate format other than Base64 encoded X.509, a third party tool such as OpenSSL can be used to convert the certificate into the appropriate format.</p>

<pre><code>openssl pkcs7 -print_certs -in old.p7b -out new.crt
#or
openssl pkcs7 -print_certs -inform der -in a.p7c -out out.cer
</code></pre>

<h2 id="create-cert-with-ca-certificate">create cert with ca certificate</h2>

<p><a href="https://www.centlinux.com/2019/01/configure-certificate-authority-ca-centos-7.html">https://www.centlinux.com/2019/01/configure-certificate-authority-ca-centos-7.html</a></p>

<pre><code>cd /etc/pki/CA/private
openssl genrsa -aes128 -out tower1CA.key 2048
openssl req -new -x509 -days 1825 -key tower1CA.key -out ../certs/tower1CA.crt
cd /etc/pki/tls/private
openssl genrsa -out tower1.key 1024
openssl req -new -key tower1.key -out tower1.csr
openssl x509 -req -in tower1.csr -CA ../../CA/certs/tower1CA.crt -CAkey ../../CA/private/tower1CA.key -CAcreateserial -out ../certs/tower1.crt -days 3650
</code></pre>

<h2 id="import-certificate">import certificate</h2>

<p><a href="https://thomas-leister.de/en/how-to-import-ca-root-certificate/">https://thomas-leister.de/en/how-to-import-ca-root-certificate/</a></p>

<h3 id="linux-debian-ubuntu-system">Linux (Debian / Ubuntu) - system</h3>

<pre><code>sudo mkdir /usr/local/share/ca-certificates/extra
sudo cp root.cert.pem /usr/local/share/ca-certificates/extra/root.cert.crt
sudo update-ca-certificates
</code></pre>

<h3 id="browser-firefox-chromium">Browser (Firefox, Chromium, …)</h3>

<pre><code>sudo apt install libnss3-tools
</code></pre>

<p>This little helper script finds trust store databases and imports the new root certificate into them.</p>

<pre><code>#!/bin/bash

### Script installs root.cert.pem to certificate trust store of applications using NSS
### (e.g. Firefox, Thunderbird, Chromium)
### Mozilla uses cert8, Chromium and Chrome use cert9

###
### Requirement: apt install libnss3-tools
###


###
### CA file to install (CUSTOMIZE!)
###

certfile=&quot;root.cert.pem&quot;
certname=&quot;My Root CA&quot;


###
### For cert8 (legacy - DBM)
###

for certDB in $(find ~/ -name &quot;cert8.db&quot;)
do
    certdir=$(dirname ${certDB});
    certutil -A -n &quot;${certname}&quot; -t &quot;TCu,Cu,Tu&quot; -i ${certfile} -d dbm:${certdir}
done


###
### For cert9 (SQL)
###

for certDB in $(find ~/ -name &quot;cert9.db&quot;)
do
    certdir=$(dirname ${certDB});
    certutil -A -n &quot;${certname}&quot; -t &quot;TCu,Cu,Tu&quot; -i ${certfile} -d sql:${certdir}
done
</code></pre>

<ul>
<li><a href="https://knowledge.digicert.com/generalinformation/INFO4448.html">https://knowledge.digicert.com/generalinformation/INFO4448.html</a></li>
<li><a href="https://www.google.com/search?q=p7b+to+crt+linux">https://www.google.com/search?q=p7b+to+crt+linux</a></li>
<li><a href="https://gist.github.com/jmervine/e4c9af3adb14b78856cc">https://gist.github.com/jmervine/e4c9af3adb14b78856cc</a></li>
<li><a href="https://manuals.gfi.com/en/kerio/connect/content/server-configuration/ssl-certificates/adding-trusted-root-certificates-to-the-server-1605.html">https://manuals.gfi.com/en/kerio/connect/content/server-configuration/ssl-certificates/adding-trusted-root-certificates-to-the-server-1605.html</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/certificate/">certificate</a>, <a href="https://blog.indonesiadot.com/tags/ssl/">ssl</a>, <a href="https://blog.indonesiadot.com/tags/csr/">csr</a>, <a href="https://blog.indonesiadot.com/tags/crt/">crt</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/03/18/ansible-tower-for-beginner/">Ansible Tower for beginner</a></h1>
        <span class="post-date">Mar 18, 2020</span>
	

<p>Before going to start the installation steps, we need to know some benefits of Ansible Tower.</p>

<h1 id="why-we-need-ansible-tower-and-it-s-benefits">Why we need Ansible Tower and it’s benefits?</h1>

<ol>
<li><p>Ansible Tower helps you scale IT automation, manage complex deployments and speed productivity.</p></li>

<li><p>Centralize and control your IT infrastructure with a visual dashboard, role-based access control, job scheduling, integrated notifications and graphical inventory management.</p></li>

<li><p>Ansible Tower’s REST API and CLI make it easy to embed Ansible Tower into existing  tools and processes.</p></li>

<li><p>The Ansible Tower dashboard provides a heads-up NOC-style display for everything  going on in your Ansible environment.</p></li>

<li><p>As soon as you log in, you’ll see your host and inventory status, all the recent job activity and a snapshot of recent job runs. Adjust your job status settings to graph data from specific job and time ranges.</p></li>

<li><p>We can get Real-Time Job Status update:</p></li>
</ol>

<p>Within Ansible Tower, playbook runs stream by in real time. As Ansible automates across your infrastructure, you’ll see plays and tasks complete, broken down by each machine, and each success or failure, complete with output. Easily see the status of your automation, and what’s next in the queue.</p>

<p>Other types of jobs, such as source control updates or cloud inventory refreshes, appear in the common job view. Know what Ansible Tower is up to at any time.</p>

<ol>
<li>Multi-Playbook Workflows:</li>
</ol>

<p>Ansible Tower Workflows allow you to easily model complex processes with Ansible Tower’s intuitive workflow editor. Ansible Tower workflows chain any number of playbooks, updates, and other workflows, regardless of whether they use different inventories, run as different users, run at once or utilize different credentials.</p>

<p>You can build a provisioning workflow that provisions machines, applies a base system configuration, and deploys an application, all with different playbooks maintained by different teams. You can build a CI/CD testing workflow that builds an application, deploys it to a test environment, runs tests, and automatically promotes the application based on test results. Set up different playbooks to run in case of success or failure of a prior workflow playbook.</p>

<ol>
<li>Who Ran What Job When:</li>
</ol>

<p>With Ansible Tower, all automation activity is securely logged. Who ran it, how they customized it, what it did, where it happened – all securely stored and viewable later, or exported through Ansible Tower’s API.</p>

<p>Activity streams extend this by showing a complete audit trail of all changes made to Ansible Tower itself – job creation, inventory changes, credential storage, all securely tracked.</p>

<p>All audit and log information can be sent to your external logging and analytics provider to perform analysis of automation and event correlation   across your entire environment.</p>

<ol>
<li><p>Ansible Tower allows us to easily streamline the delivery of applications and services to both OpenStack and Amazon Clouds in a cost effective, simple, and secure manner.</p></li>

<li><p>Scale Capacity With Tower Clusters:</p></li>
</ol>

<p>Connect multiple Ansible Tower nodes into a Ansible Tower cluster. Ansible Tower clusters add redundancy and capacity, allowing you to scale Ansible automation across your enterprise, including with reserved capacity for certain teams and jobs, and remote execution for access across network zones.</p>

<ol>
<li>Integrated Notifications:</li>
</ol>

<p>Stay informed of your automation status via integrated notifications.</p>

<p>Notify a person or team when your job succeeds, or escalate when jobs fail. Send notifications across your entire organization at once, or customize on a per- job basis.</p>

<p>Connect your notifications to Slack, Hipchat, PagerDuty, SMS, email, and more – or post notifications to a custom webhook to trigger other tools in your infrastructure.</p>

<ol>
<li>Schedule Ansible Jobs:</li>
</ol>

<p>Playbook runs, cloud inventory updates, and source control updates can be scheduled inside Ansible Tower – run now, run later, or run forever.</p>

<p>Set up occasional tasks like nightly backups, periodic configuration remediation for compliance, or a full continuous delivery pipeline with just a few clicks.</p>

<ol>
<li>Manage And Track Your Entire Inventory:</li>
</ol>

<p>Ansible Tower helps you manage your entire infrastructure. Easily pull your  inventory from public cloud providers such as Amazon Web Services, Microsoft Azure, and more, or synchronize from your local OpenStack cloud or VMware environment. Connect your inventory directly to your Red Hat Satellite or Red Hat CloudForms environment, or your custom CMDB.</p>

<p>Ansible Tower can keep your cloud inventory in sync, and Ansible Tower’s powerful provisioning callbacks allow nodes to request configuration on demand, enabling autoscaling. You can also see alerts from Red Hat Insights directly from Ansible Tower, and use Insights-provided Playbook Remediation to fix issues in your infrastructure.</p>

<p>Plus, Ansible Tower Smart Inventories allow you to organize and automate hosts across all your providers based on a powerful host fact query engine.</p>

<ol>
<li>Remote Command Execution:</li>
</ol>

<p>Run simple tasks on any host or group of hosts in your inventory with Ansible Tower’s remote command execution. Add users or groups, reset passwords, restart a malfunctioning service or patch a critical security issue, quickly. As always, remote command execution uses Ansible Tower’s role-based access control engine and logs every action.</p>

<ol>
<li>Comprehensive Rest API And Tower CLI Tool:</li>
</ol>

<p>Far from being limited to just the user interface, every feature of Ansible Tower is available via Ansible Tower’s REST API, providing the ideal API for a systems management infrastructure to build against. Call Ansible Tower jobs from your build tools, show Ansible Tower information in your custom dashboards and more. Get API usage information and best practices with built-in documentation.</p>

<h2 id="prerequisites">Prerequisites:</h2>

<p>Ansible Tower server (I’m using a VMware environment, so both my servers are VMs)</p>

<p>1 Core, 1GB RAM Ubuntu 12.04 LTS Server, 64-bit</p>

<p>Active Directory Server (I’m using Windows Server 2012 R2)</p>

<p>2 Cores, 4GB RAM</p>

<p>Officially, Tower supports CentOS 6, RedHat Enterprise Linux 6, Ubuntu Server 12.04 LTS, and Ubuntu Server 14.04 LTS.</p>

<p>Installing Tower requires Internet connectivity, because it downloads from their repo servers.</p>

<p>I have managed to perform an offline installation, but you have to set up some kind of system to mirror their repositories, and change some settings in the Ansible Installer file.</p>

<p>I <em>highly</em> recommend you dedicate a server (vm or otherwise) to Ansible Tower, because the installer will rewrite pg_hba.conf and supervisord.conf to suit its needs.  Everything is easier if you give it it’s own environment to run in.</p>

<p>You <em>might</em> be able to do it in Docker, although I haven’t tried, and I’m willing to bet you’re asking for trouble.</p>

<p>I’m going to assume you already know about installing Windows Server 2012 and building a domain controller. (If there’s significant call for it, I might write a separate blog post about this…)</p>

<h1 id="installation-steps">Installation Steps:</h1>

<h2 id="step-1">Step – 1:</h2>

<p>Download the latest .tar file from ‘<a href="https://releases.ansible.com/ansible-tower/setup/&amp;#8217">https://releases.ansible.com/ansible-tower/setup/&amp;#8217</a>;</p>

<h2 id="step-2">Step – 2:</h2>

<p>Now untar the downloaded .tar file using below command:</p>

<p>[devops@localhost ~]$ tar xvzf ansible-tower-setup-latest.tar.gz</p>

<h2 id="step-3">Step – 3:</h2>

<p>goto “ansible-tower-setup-<VERSION>” directory</p>

<p>[devops@localhost ~]$ cd ansible-tower-setup-3.4.0-2</p>

<h2 id="step-4">Step – 4:</h2>

<p>edit inventory file which is present inside “ansible-tower-setup-3.4.0-2″ directory.</p>

<p>Put the below content to inventory file:</p>

<pre><code># /home/ubuntu/ansible-tower-setup-3.2.5/inventory file content

[tower]
localhost ansible_connection=local

[database]

[all:vars]
admin_password=’admin’

pg_host=”
pg_port=”

pg_database=’awx’
pg_username=’awx’
pg_password=’admin’

rabbitmq_port=5672
rabbitmq_vhost=tower
rabbitmq_username=tower
rabbitmq_password=’admin’
rabbitmq_cookie=cookiemonster

# Needs to be true for fqdns and ip addresses
rabbitmq_use_long_name=false

# Isolated Tower nodes automatically generate an RSA key for authentication;
# To disable this behavior, set this value to false
# isolated_key_generation=true
################################# Up to here ########################
</code></pre>

<h2 id="step-5">Step – 5:</h2>

<p>Now Run the ansible-tower setup by using below command:</p>

<p>[devops@localhost ansible-tower-setup-3.4.0-2]$ sudo ansible-playbook -i inventory install.yml
[INFO]  To Run Above command we should have ansible installed on this server</p>

<p>[INFO]  If ansible is not present then please open the below link and install the ansible:</p>

<p>[INFO]  <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-centos-7">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-centos-7</a></p>

<p>[INFO] SetUp will take approx 45 mins or more.</p>

<p>Sample Output:</p>

<pre><code>[sudo] password for devops:
[WARNING]: Could not match supplied host pattern, ignoring: instance_group_*
PLAY [tower:database:instance_group_*] ******************************************************************************************************************************

TASK [check_config_static : Ensure expected variables are defined] **************************************************************************************************
skipping: [localhost] =&gt; (item=tower_package_name)
skipping: [localhost] =&gt; (item=tower_package_version)
skipping: [localhost] =&gt; (item=tower_package_release)

TASK [check_config_static : Detect unsupported HA inventory file] ***************************************************************************************************
skipping: [localhost]

TASK [check_config_static : Ensure at least one tower host is defined] **********************************************************************************************
skipping: [localhost]

TASK [check_config_static : Ensure only one database host exists] ***************************************************************************************************
skipping: [localhost]

TASK [check_config_static : Ensure when postgres host is defined that the port is defined] **************************************************************************
skipping: [localhost]

TASK [check_config_static : Ensure that when a database host is specified, that pg_host is defined] *****************************************************************
skipping: [localhost]

TASK [check_config_static : Ensure that when a database host is specified, that pg_port is defined] *****************************************************************
skipping: [localhost]

TASK [check_config_static : HA mode requires an external postgres database with pg_host defined] ********************************************************************
skipping: [localhost]

TASK [check_config_static : HA mode requires an external postgres database with pg_port defined] ********************************************************************
skipping: [localhost]

TASK [config_dynamic : Set database to internal or external] ********************************************************************************************************
ok: [localhost]

TASK [config_dynamic : Database decision] ***************************************************************************************************************************
ok: [localhost] =&gt; {
“config_dynamic_database”: “internal”
}

TASK [config_dynamic : Set postgres host and port to local if not set] **********************************************************************************************
ok: [localhost]

TASK [config_dynamic : Ensure connectivity to hosts and gather facts] ***********************************************************************************************
ok: [localhost]

TASK [config_dynamic : Get effective uid] ***************************************************************************************************************************
changed: [localhost]

TASK [config_dynamic : Ensure user is root] *************************************************************************************************************************
skipping: [localhost]

PLAY [Group nodes by OS distribution] *******************************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************************************
ok: [localhost]

TASK [group hosts by distribution] **********************************************************************************************************************************
ok: [localhost]
[WARNING]: Could not match supplied host pattern, ignoring: RedHat-7*

[WARNING]: Could not match supplied host pattern, ignoring: Ubuntu-16.04

[WARNING]: Could not match supplied host pattern, ignoring: OracleLinux-7*
PLAY [Group supported distributions] ********************************************************************************************************************************

TASK [group hosts for supported distributions] **********************************************************************************************************************
ok: [localhost]
[WARNING]: Could not match supplied host pattern, ignoring: none

.

.

.

.

TASK [misc : Create the default organization if it is needed.] ******************************************************************************************************
changed: [localhost]

RUNNING HANDLER [supervisor : restart supervisor] *******************************************************************************************************************
changed: [localhost] =&gt; {
“msg”: “Restarting supervisor.”
}

RUNNING HANDLER [supervisor : Stop supervisor.] *********************************************************************************************************************
changed: [localhost]

RUNNING HANDLER [supervisor : Wait for supervisor to stop.] *********************************************************************************************************
ok: [localhost]

RUNNING HANDLER [supervisor : Start supervisor.] ********************************************************************************************************************
changed: [localhost]

RUNNING HANDLER [nginx : restart nginx] *****************************************************************************************************************************
changed: [localhost]

PLAY [Install Tower isolated node(s)] *******************************************************************************************************************************
skipping: no hosts matched

PLAY RECAP **********************************************************************************************************************************************************
localhost : ok=139 changed=65 unreachable=0 failed=0

########### Sample Output Ends Here ############
</code></pre>

<h2 id="step-6-now-open-browser-and-type-below">Step – 6: Now, open browser and type below:</h2>

<p><a href="https://Your-Server-IP-Address">https://Your-Server-IP-Address</a>  and hit enter.</p>

<p>Example:</p>

<p><a href="https://192.168.56.102">https://192.168.56.102</a></p>

<p>1.Browser_Proceed_unsafe.png</p>

<p>Now, Click on –&gt; Advance–&gt;click on–&gt;proceed to <192.168.56.102> –&gt;It will show as below</p>

<p>It will automatically redirect to Ansible Tower page(as shown below)</p>

<p>2.Login_Page_on_browser.png</p>

<p>Now, Put your username and password(which you have provided in “/home/ubuntu/ansible-tower-setup-3.2.5/inventory” file)</p>

<p>And click on “SIGN IN” button.</p>

<p>After logged-in successfully, It will ask for license.Please click on “REQUEST LICENSE”3.Request_license.png</p>

<p>It will take to you on ansible tower license request page.Please select the appropriate options and fill the all mandatory fields and click on “Submit” button as shown below:</p>

<p>4.Redirect_to_get_license_after_filling_proper_details_main.png</p>

<p>After Click on “Submit” button, it will send the license file to your mail within 1 business day. Please check the mail.</p>

<p>After getting the license file, Please open the Ansible tower and do below things which are marked as Orange Square:</p>

<p><img src="https://blog.indonesiadot.com/images/2020/3.1browse_license_file.png" alt="license" /></p>

<p>And click on “Submit” button.</p>

<p>After successful submission, It will show you the Ansible Tower Dashboard as shown below:</p>

<p><img src="https://blog.indonesiadot.com/images/2020/5.After_successful_login_first_page_view.png" alt="login" /></p>

<p>Finally, You have successful Ansible Tower installation on Centos 7 Machine.</p>

<p>In the next blog we will see how to run the first playbook from Ansible Tower.</p>

<p>Referensi:</p>

<ul>
<li><a href="https://mohitepramod.wordpress.com/2019/01/19/ansible-tower-installation/">https://mohitepramod.wordpress.com/2019/01/19/ansible-tower-installation/</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-centos-7">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-centos-7</a></li>
<li><a href="https://releases.ansible.com/ansible-tower/docs/tower_user_guide-2.1.6.pdf">https://releases.ansible.com/ansible-tower/docs/tower_user_guide-2.1.6.pdf</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/ansible/">ansible</a>, <a href="https://blog.indonesiadot.com/tags/ansibletower/">ansibletower</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/03/18/elk-for-beginner/">ELK for beginner</a></h1>
        <span class="post-date">Mar 18, 2020</span>
	

<h1 id="why-we-use-elk">Why we use ELK?</h1>

<p>The Elastic Stack (aka ELK) is a robust solution for search, log management, and data analysis. ELK consists of a combination of three open source project: Elasticsearch, Logstash, and Kibana. These projects have specific roles in ELK:</p>

<p>Elasticsearch handles storage and provides a RESTful search and analytics endpoint.
Logstash is a server-side data processing pipeline that ingests, transforms and loads data.
Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack.</p>

<ol>
<li>Elasticsearch -The Amazing Log Search Tool:</li>
</ol>

<p>Real-time data extraction, and real-time data analytics. Elasticsearch is the engine that gives you both the power and the speed.
2.Logstash — Routing Your Log Data:</p>

<p>Logstash is a tool for log data intake, processing, and output. This includes virtually any type of log that you manage: system logs, webserver logs, error logs, and app logs.
As administrators, we know how much time can be spent normalizing data from disparate data sources.
We know, for example, how widely Apache logs differ from NGINX logs.
3.Kibana — Visualizing Your Log Data:</p>

<p>Kibana is your log-data dashboard.
Get a better grip on your large data stores with point-and-click pie charts, bar graphs, trendlines, maps and scatter plots.
You can visualize trends and patterns for data that would otherwise be extremely tedious to read and interpret.</p>

<h1 id="benefits">Benefits:</h1>

<p>Real-time data and real-time analytics::
The ELK stack gives you the power of real-time data insights, with the ability to perform super-fast data extractions from virtually all structured or unstructured data sources.
Real-time extraction, and real-time analytics. Elasticsearch is the engine that gives you both the power and the speed.
2. Scalable, high-availability, multi-tenant:</p>

<p>With Elasticsearch, you can start small and expand it along with your business growth-when you are ready.
It is built to scale horizontally out of the box. As you need more capacity, simply add another node and let the cluster reorganize itself to accommodate and exploit the extra hardware.
Elasticsearch clusters are resilient, since they automatically detect and remove node failures.
You can set up multiple indices and query each of them independently or in combination.</p>

<p>Some Important Concepts In ELK as follows:</p>

<ol>
<li><p>Documents:
Documents are JSON objects that are stored within an Elasticsearch index and are    considered the base unit of storage.
In the world of relational databases, documents can be compared to a row in table    Data in documents is defined with fields comprised of keys and values.
A key is the name of the field, and a value can be an item of many different types such as a string, a number, a boolean expression, another object, or an array of values.
Documents also contain reserved fields that constitute the document metadata such as:</p>

<pre><code>1.  _index – the index where the document resides
2. _type – the type that the document represents
3.  _id – the unique identifier for the document
</code></pre>

<h2 id="2-index">2.Index:</h2>

<p>Indices are the largest unit of data in Elasticsearch, are logical partitions of documents and can be compared to a database in the world of relational databases.
You can have as many indices defined in Elasticsearch as you want.
These in turn will hold documents that are unique to each index.
Indices are identified by lowercase names that refer to actions that are performed actions (such as searching and deleting)against the documents that are inside each index.
3.Shards:
Elasticsearch provides the ability to subdivide your index into multiple pieces called shards.
When you create an index, you can simply define the number of shards that you want.
Each shard is in itself a fully-functional and independent “index” that can be hosted on any node in the cluster.
When you create an index, you can define how many shards you want. Each shard is an independent Lucene index that can be hosted anywhere in your cluster.</p></li>
</ol>

<p>Sharding is important for two primary reasons:</p>

<p>It allows you to horizontally split/scale your content volume.
It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput.
example :</p>

<pre><code>curl -XPUT localhost:9200/example -d ‘{
“settings” : {
“index” : {
“number_of_shards” : 2,
“number_of_replicas” : 1
}
}
}’
</code></pre>

<h2 id="4-replicas">4.Replicas:</h2>

<p>Replicas, as the name implies, are Elasticsearch fail-safe mechanisms and are basically copies of your index’s shards.
This is a useful backup system for a rainy day — or, in other words, when a node crashes.
Replicas also serve read requests, so adding replicas can help to increase search performance.
To ensure high availability, replicas are not placed on the same node as the original shards (called the “primary” shard) from which they were replicated.</p>

<p>To ensure high availability, replicas are not placed on the same node as the original shards (called the “primary” shard)from which they were replicated.</p>

<p>Replication is important for two primary reasons:</p>

<p>It provides high availability in case a shard/node fails. For this reason,
it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from.
It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel.</p>

<h2 id="5-analyzers">5.Analyzers:</h2>

<p>Analyzers are used during indexing to break down phrases or expressions into terms.
Defined within an index, an analyzer consists of a single tokenizer and any number of token filters.
For example, a tokenizer could split a string into specifically defined terms when encountering a specific expression.</p>

<p>A token filter is used to filter or modify some tokens. For example, a ASCII folding filter will convert characters like ê, é, è to e.
example:</p>

<pre><code>curl -XPUT localhost:9200/example -d ‘{
“mappings”: {
“mytype”: {
“properties”: {
“name”: {
“type”: “string”,
“analyzer”: “whitespace”
}
}
}
}
}’
</code></pre>

<h2 id="6-nodes">6.Nodes:</h2>

<p>The heart of any ELK setup is the Elasticsearch instance, which has the crucial task of storing and indexing data.</p>

<p>In a cluster, different responsibilities are assigned to the various node types:
1.Data nodes — stores data and executes data-related operations such as search and aggregation.
2.Master nodes — in charge of cluster-wide management and configuration actions such as adding and removing nodes
3.Client nodes — forwards cluster requests to the master node and data-related requests to data nodes
4.Tribe nodes — act as a client node, performing read and write operations against all of the nodes in the cluster
5.Ingestion nodes (this is new in Elasticsearch 5.0) — for pre-processing documents before indexing</p>

<p>By default, each node is automatically assigned a unique identifier, or name, that is used for management purposes and becomes even more important in a multi-node, or clustered, environment.</p>

<p>When installed, a single node will form a new single-node cluster entitled elasticsearch,” but it can also be configured to join an existing cluster (see below) using the cluster name.</p>

<p>In a development or testing environment, you can set up multiple nodes on a single server.
In production, however, due to the number of resources that an Elasticsearch node consumes,
it is recommended to have each Elasticsearch instance run on a separate server.</p>

<h2 id="7-cluster">7.Cluster:</h2>

<p>An Elasticsearch cluster is comprised of one or more Elasticsearch nodes.
As with nodes, each cluster has a unique identifier that must be used by any node attempting to join the cluster.
By default, the cluster name is “elasticsearch,” but this name can be changed, of course.</p>

<p>One node in the cluster is the “master” node, which is in charge of cluster-wide management and configurations actions (such as adding and removing nodes).</p>

<p>This node is chosen automatically by the cluster, but it can be changed if it fails. (See above on the other types of nodes in a cluster.)</p>

<p>For example, the cluster health API returns health status reports of either “green” (all shards are allocated), “yellow” (the primary shard is allocated but replicas are not), or “red” (the shard is not allocated in the cluster).</p>

<h2 id="output-example">Output Example</h2>

<pre><code>{
“cluster_name” : “elasticsearch”,
“status” : “yellow”,
“timed_out” : false,
“number_of_nodes” : 1,
“number_of_data_nodes” : 1,
“active_primary_shards” : 5,
“active_shards” : 5,
“relocating_shards” : 0,
“initializing_shards” : 0,
“unassigned_shards” : 5,
“delayed_unassigned_shards” : 0,
“number_of_pending_tasks” : 0,
“number_of_in_flight_fetch” : 0,
“task_max_waiting_in_queue_millis” : 0,
“active_shards_percent_as_number” : 50.0
}
</code></pre>

<h1 id="elk-installation">ELK Installation:</h1>

<p>Our Goal:</p>

<p>The goal of the tutorial is to set up Logstash to gather syslogs of multiple servers, and set up Kibana to visualize the gathered logs.</p>

<p>Our ELK stack setup has four main components:</p>

<p>Logstash: The server component of Logstash that processes incoming logs
Elasticsearch: Stores all of the logs
Kibana: Web interface for searching and visualizing logs, which will be proxied through Nginx
Filebeat: Installed on client servers that will send their logs to Logstash, Filebeat serves as a log shipping agent that utilizes the lumberjack networking protocol to communicate with Logstash
<img src="https://blog.indonesiadot.com/images/2020/ELK_Architecture.png" alt="elkarch" /></p>

<p>NOTE:</p>

<p>We will install the first three components on a single server, which we will refer to as our ELK Server. Filebeat will be installed on all of the client servers that we want to gather logs for, which we will refer to collectively as our Client Servers.</p>

<h2 id="pre-requisites">Pre-requisites:</h2>

<p>The amount of CPU, RAM, and storage that your ELK Server will require depends on the volume of logs that you intend to gather. For this tutorial, we will be using a VPS with the following specs for our ELK Server:</p>

<p>OS: Ubuntu 14.04
RAM: 4GB
CPU: 2
In addition to your ELK Server, you will want to have a few other servers that you will gather logs from.</p>

<p>Let’s get started on setting up our ELK Server!</p>

<h1 id="step-1-install-java8">Step-1 : Install Java8</h1>

<p>Elasticsearch and Logstash require Java, so we will install that now. We will install a recent version of Oracle Java 8 because that is what Elasticsearch recommends. It should, however, work fine with OpenJDK, if you decide to go that route.</p>

<p>Add the Oracle Java PPA to apt:</p>

<pre><code>$ sudo add-apt-repository -y ppa:webupd8team/java
</code></pre>

<p>Update your apt package database:</p>

<pre><code>$ sudo apt-get update -y
</code></pre>

<p>Install the latest stable version of Oracle Java 8 with this command (and accept the license agreement that pops up):</p>

<pre><code>$ sudo apt-get -y install oracle-java8-installer
</code></pre>

<p>Now that Java 8 is installed.</p>

<p>let’s install ElasticSearch.</p>

<h1 id="step-2-install-elasticsearch">Step-2: Install ElasticSearch</h1>

<p>Elasticsearch can be installed with a package manager by adding Elastic’s package source list.</p>

<p>Run the following command to import the Elasticsearch public GPG key into apt:</p>

<pre><code>$ wget -qO – https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add –
</code></pre>

<p>If your prompt is just hanging there, it is probably waiting for your user’s password (to authorize the sudocommand). If this is the case, enter your password.</p>

<p>Create the Elasticsearch source list:</p>

<pre><code>$ echo “deb http://packages.elastic.co/elasticsearch/2.x/debian stable main” | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list
</code></pre>

<p>Update your apt package database:</p>

<pre><code>$ sudo apt-get update -y
</code></pre>

<p>Install Elasticsearch with this command:</p>

<pre><code>$ sudo apt-get -y install elasticsearch
</code></pre>

<p>Elasticsearch is now installed. Let’s edit the configuration:</p>

<pre><code>$ sudo vi /etc/elasticsearch/elasticsearch.yml
</code></pre>

<p>You will want to restrict outside access to your Elasticsearch instance (port 9200), so outsiders can’t read your data or shutdown your Elasticsearch cluster through the HTTP API. Find the line that specifies network.host, uncomment it, and replace its value with “localhost” so it looks like this:</p>

<p>elasticsearch.yml excerpt (updated)</p>

<pre><code>network.host: localhost
</code></pre>

<p>Save and exit elasticsearch.yml.</p>

<p>Now start Elasticsearch:</p>

<pre><code>$ sudo service elasticsearch restart
</code></pre>

<p>Then run the following command to start Elasticsearch on boot up:</p>

<pre><code>$ sudo update-rc.d elasticsearch defaults 95 10
</code></pre>

<p>Now that Elasticsearch is up and running, let’s install Kibana.</p>

<h1 id="step-3-install-kibana">Step-3:Install Kibana</h1>

<p>Kibana can be installed with a package manager by adding Elastic’s package source list.</p>

<p>Create the Kibana source list:</p>

<pre><code>$ echo “deb http://packages.elastic.co/kibana/4.5/debian stable main” | sudo tee -a /etc/apt/sources.list.d/kibana-4.5.x.list
</code></pre>

<p>Update your apt package database:</p>

<pre><code>sudo apt-get update -y
</code></pre>

<p>Install Kibana with this command:</p>

<pre><code>sudo apt-get -y install kibana
</code></pre>

<p>Kibana is now installed.</p>

<p>Open the Kibana configuration file for editing:</p>

<pre><code>$ sudo vi /opt/kibana/config/kibana.yml
</code></pre>

<p>In the Kibana configuration file, find the line that specifies server.host, and replace the IP address (“0.0.0.0” by default) with “localhost”:</p>

<pre><code>server.host: &quot;localhost&quot;
</code></pre>

<p>Save and exit. This setting makes it so Kibana will only be accessible to the localhost. This is fine because we will use an Nginx reverse proxy to allow external access.</p>

<p>Now enable the Kibana service, and start it:</p>

<pre><code>sudo update-rc.d kibana defaults 96 9
sudo service kibana start
</code></pre>

<p>Before we can use the Kibana web interface, we have to set up a reverse proxy. Let’s do that now, with Nginx.</p>

<h1 id="step-4-install-nginx">Step-4:Install Nginx</h1>

<p>Because we configured Kibana to listen on localhost, we must set up a reverse proxy to allow external access to it. We will use Nginx for this purpose.</p>

<p>Note: If you already have an Nginx instance that you want to use, feel free to use that instead. Just make sure to configure Kibana so it is reachable by your Nginx server (you probably want to change the hostvalue, in /opt/kibana/config/kibana.yml, to your Kibana server’s private IP address or hostname). Also, it is recommended that you enable SSL/TLS.</p>

<p>Use apt to install Nginx and Apache2-utils:</p>

<pre><code>$ sudo apt-get install nginx apache2-utils -y
</code></pre>

<p>Use htpasswd to create an admin user, called “kibanaadmin” (you should use another name), that can access the Kibana web interface:</p>

<pre><code>$ sudo htpasswd -c /etc/nginx/htpasswd.users kibanaadmin
</code></pre>

<p>Enter a password at the prompt. Remember this login, as you will need it to access the Kibana web interface.</p>

<p>Now open the Nginx default server block in your favorite editor. We will use vi:</p>

<pre><code>$ sudo vim /etc/nginx/sites-available/default
</code></pre>

<p>Delete the file’s contents, and paste the following code block into the file. Be sure to update the server_name to match your server’s name:</p>

<pre><code>server {

listen 80;

server_name example.com;

auth_basic “Restricted Access”;

auth_basic_user_file /etc/nginx/htpasswd.users;

location / {

proxy_pass http://localhost:5601;

proxy_http_version 1.1;

proxy_set_header Upgrade $http_upgrade;

proxy_set_header Connection ‘upgrade’;

proxy_set_header Host $host;

proxy_cache_bypass $http_upgrade;

}

}
</code></pre>

<p>Nginx configuration look like :</p>

<p><img src="https://blog.indonesiadot.com/images/2020/nginx.png" alt="nginx" /></p>

<p>Save and exit. This configures Nginx to direct your server’s HTTP traffic to the Kibana application, which is listening on localhost:5601. Also, Nginx will use the htpasswd.users file, that we created earlier, and require basic authentication.</p>

<p>Now restart Nginx to put our changes into effect:</p>

<pre><code>$ sudo service nginx restart
</code></pre>

<p>Kibana is now accessible via your FQDN or the public IP address of your ELK Server i.e. <a href="http://elk-server-public-ip/">http://elk-server-public-ip/</a>. If you go there in a web browser, after entering the “kibanaadmin” credentials, you should see a Kibana welcome page which will ask you to configure an index pattern. Let’s get back to that later, after we install all of the other components.</p>

<h1 id="step-5-install-logstash">Step-5 Install Logstash</h1>

<p>The Logstash package is available from the same repository as Elasticsearch, and we already installed that public key, so let’s create the Logstash source list:</p>

<pre><code>$ echo ‘deb http://packages.elastic.co/logstash/2.2/debian stable main’ | sudo tee /etc/apt/sources.list.d/logstash-2.2.x.list
</code></pre>

<p>Update your apt package database:</p>

<pre><code>$ sudo apt-get update -y
</code></pre>

<p>Install Logstash with this command:</p>

<pre><code>$ sudo apt-get install logstash -y
</code></pre>

<p>Logstash is installed but it is not configured yet.
Since we are going to use Filebeat to ship logs from our Client Servers to our ELK Server, we need to create an SSL certificate and key pair. The certificate is used by Filebeat to verify the identity of ELK Server. Create the directories that will store the certificate and private key with the following commands:</p>

<pre><code>sudo mkdir -p /etc/pki/tls/certs
sudo mkdir /etc/pki/tls/private
</code></pre>

<p>Now you have two options for generating your SSL certificates. If you have a DNS setup that will allow your client servers to resolve the IP address of the ELK Server, use Option 2. Otherwise, Option 1 will allow you to use IP addresses.</p>

<h2 id="option-1-ip-address">Option 1:IP Address</h2>

<p>If you don’t have a DNS setup—that would allow your servers, that you will gather logs from, to resolve the IP address of your ELK Server—you will have to add your ELK Server’s private IP address to the subjectAltName (SAN) field of the SSL certificate that we are about to generate. To do so, open the OpenSSL configuration file:</p>

<pre><code>$ sudo vim /etc/ssl/openssl.cnf
</code></pre>

<p>Find the [ v3_ca ] section in the file, and add this line under it (substituting in the ELK Server’s private IP address):</p>

<pre><code>subjectAltName = IP: ELK_server_private_IP
</code></pre>

<p>Save and exit.</p>

<p>Now generate the SSL certificate and private key in the appropriate locations (/etc/pki/tls/), with the following commands:</p>

<pre><code>cd /etc/pki/tls
sudo openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
</code></pre>

<p>The logstash-forwarder.crt file will be copied to all of the servers that will send logs to Logstash but we will do that a little later. Let’s complete our Logstash configuration. If you went with this option, skip option 2 and move on to Configure Logstash.</p>

<h2 id="option-2-fqdn-dns">Option 2: FQDN(DNS)</h2>

<p>If you have a DNS setup with your private networking, you should create an A record that contains the ELK Server’s private IP address—this domain name will be used in the next command, to generate the SSL certificate. Alternatively, you can use a record that points to the server’s public IP address. Just be sure that your servers (the ones that you will be gathering logs from) will be able to resolve the domain name to your ELK Server.</p>

<p>Now generate the SSL certificate and private key, in the appropriate locations (/etc/pki/tls/…), with the following command (substitute in the FQDN of the ELK Server):</p>

<pre><code>$ cd /etc/pki/tls; sudo openssl req -subj ‘/CN=ELK_server_fqdn/’ -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
</code></pre>

<p>The logstash-forwarder.crt file will be copied to all of the servers that will send logs to Logstash but we will do that a little later. Let’s complete our Logstash configuration.</p>

<h2 id="configure-logstash">Configure Logstash</h2>

<p>Logstash configuration files are in the JSON-format, and reside in /etc/logstash/conf.d. The configuration consists of three sections: inputs, filters, and outputs.</p>

<p>Let’s create a configuration file called 02-beats-input.conf and set up our “filebeat” input:</p>

<pre><code>$ sudo vi /etc/logstash/conf.d/02-beats-input.conf
Insert the following input configuration:

input {

beats {

port =&gt; 5044

ssl =&gt; true

ssl_certificate =&gt; “/etc/pki/tls/certs/logstash-forwarder.crt”

ssl_key =&gt; “/etc/pki/tls/private/logstash-forwarder.key”

}

}
</code></pre>

<p>Or</p>

<p>02-beats-input.conf file content looks like:</p>

<p><img src="https://blog.indonesiadot.com/images/2020/logstash1.png" alt="logstash1" /></p>

<p>Save and quit. This specifies a beats input that will listen on tcp port 5044, and it will use the SSL certificate and private key that we created earlier.</p>

<p>Now let’s create a configuration file called 10-syslog-filter.conf, where we will add a filter for syslog messages:</p>

<pre><code>$ sudo vi /etc/logstash/conf.d/10-syslog-filter.conf
filter {

if [type] == “syslog” {

grok {

match =&gt; { “message” =&gt; “%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}” }

add_field =&gt; [ “received_at”, “%{@timestamp}” ]

add_field =&gt; [ “received_from”, “%{host}” ]

}

syslog_pri { }

date {

match =&gt; [ “syslog_timestamp”, “MMM d HH:mm:ss”, “MMM dd HH:mm:ss” ]

}

}

}
</code></pre>

<p>Or 10-syslog-filter.conf file content looks like this:</p>

<p><img src="logstash2" alt="logstash2" /></p>

<p>Save and quit. This filter looks for logs that are labeled as “syslog” type (by Filebeat), and it will try to use grok to parse incoming syslog logs to make it structured and query-able.</p>

<p>Lastly, we will create a configuration file called 30-elasticsearch-output.conf:</p>

<pre><code>$ sudo vim /etc/logstash/conf.d/30-elasticsearch-output.conf
output {
elasticsearch {

hosts =&gt; [“localhost:9200”]
sniffing =&gt; true
manage_template =&gt; false
index =&gt; “%{[@metadata][beat]}-%{+YYYY.MM.dd}”
document_type =&gt; “%{[@metadata][type]}”

}

}
</code></pre>

<p>Or 30-elasticsearch-output.conf file content looks like:</p>

<p><img src="https://blog.indonesiadot.com/images/2020/logstash3.png" alt="logstash3" /></p>

<p>Save and exit. This output basically configures Logstash to store the beats data in Elasticsearch which is running at localhost:9200, in an index named after the beat used (filebeat, in our case).</p>

<p>If you want to add filters for other applications that use the Filebeat input, be sure to name the files so they sort between the input and the output configuration (i.e. between 02- and 30-).</p>

<p>Test your Logstash configuration with this command:</p>

<pre><code>$ sudo service logstash configtest
</code></pre>

<p>It should display Configuration OK if there are no syntax errors. Otherwise, try and read the error output to see what’s wrong with your Logstash configuration.</p>

<p>Restart Logstash, and enable it, to put our configuration changes into effect:</p>

<pre><code>sudo service logstash restart
sudo update-rc.d logstash defaults 96 9
</code></pre>

<p>Next, we’ll load the sample Kibana dashboards.</p>

<p>##Loading Sample Kibana Dashboards:</p>

<p>Elastic provides several sample Kibana dashboards and Beats index patterns that can help you get started with Kibana. Although we won’t use the dashboards in this tutorial, we’ll load them anyway so we can use the Filebeat index pattern that it includes.</p>

<p>First, download the sample dashboards archive to your home directory:</p>

<pre><code>cd ~
curl -L -O https://download.elastic.co/beats/dashboards/beats-dashboards-1.1.0.zip
</code></pre>

<p>Install the unzip package with this command:</p>

<pre><code>sudo apt-get -y install unzip
</code></pre>

<p>Next, extract the contents of the archive:</p>

<pre><code>unzip beats-dashboards-*.zip
</code></pre>

<p>And load the sample dashboards, visualizations and Beats index patterns into Elasticsearch with these commands:</p>

<pre><code>cd beats-dashboards-*
./load.sh
</code></pre>

<p>These are the index patterns that we just loaded:</p>

<p>[packetbeat-]YYYY.MM.DD
[topbeat-]YYYY.MM.DD
[filebeat-]YYYY.MM.DD
[winlogbeat-]YYYY.MM.DD
When we start using Kibana, we will select the Filebeat index pattern as our default.</p>

<p>Load Filebeat index templates in Elasticsearch
Because we are planning on using Filebeat to ship logs to Elasticsearch, we should load a Filebeat index template. The index template will configure Elasticsearch to analyze incoming Filebeat fields in an intelligent way.</p>

<p>First, download the Filebeat index template to your home directory:</p>

<pre><code>cd ~
curl -O https://gist.githubusercontent.com/thisismitch/3429023e8438cc25b86c/raw/d8c479e2a1adcea8b1fe86570e42abab0f10f364/filebeat-index-template.json
</code></pre>

<p>Then load the template with this command:</p>

<pre><code>$ curl -XPUT ‘http://localhost:9200/_template/filebeat?pretty&amp;#8217; -d@filebeat-index-template.json
</code></pre>

<p>If the template loaded properly, you should see a message like this:</p>

<p>Output:
{
  &ldquo;acknowledged&rdquo; : true
}
Now that our ELK Server is ready to receive Filebeat data, let’s move onto setting up Filebeat on each client server.</p>

<p>Step-6: SetUp filebeat(add clients servers)
Do these steps for each Ubuntu or Debian server that you want to send logs to Logstash on your ELK Server. For instructions on installing Filebeat on Red Hat-based Linux distributions (e.g. RHEL, CentOS, etc.), refer to the Set Up Filebeat (Add Client Servers) section of the CentOS variation of this tutorial.</p>

<p>Copy SSL Certificate
On your ELK Server, copy the SSL certificate—created in the prerequisite tutorial—to your Client Server(substitute the client server’s address, and your own login):</p>

<pre><code>scp /etc/pki/tls/certs/logstash-forwarder.crt user@client_server_private_address:/tmp
</code></pre>

<p>After providing your login’s credentials, ensure that the certificate copy was successful. It is required for communication between the client servers and the ELK Server.</p>

<p>Now, on your Client Server, copy the ELK Server’s SSL certificate into the appropriate location (/etc/pki/tls/certs):</p>

<pre><code>Client $ sudo mkdir -p /etc/pki/tls/certs
CLient$ sudo cp /tmp/logstash-forwarder.crt /etc/pki/tls/certs/
</code></pre>

<p>Now we will install the Topbeat package.</p>

<p>Install Filebeat packages:
On Client Server, create the Beats source list,using following command :</p>

<pre><code>Client$ echo “deb https://packages.elastic.co/beats/apt stable main” | sudo tee -a /etc/apt/sources.list.d/beats.list
</code></pre>

<p>It also uses the same GPG key as Elasticsearch, which can be installed with this command:</p>

<pre><code>Client$ wget -qO – https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add –
</code></pre>

<p>Then install the Filebeat package:</p>

<pre><code>sudo apt-get update
sudo apt-get install filebeat
Filebeat is installed but it is not configured yet.
</code></pre>

<p>Configure filebeat
Now we will configure Filebeat to connect to Logstash on our ELK Server. This section will step you through modifying the example configuration file that comes with Filebeat. When you complete the steps, you should have a file that looks something like this.</p>

<p>On Client Server, create and edit Filebeat configuration file:</p>

<pre><code>sudo vi /etc/filebeat/filebeat.yml
</code></pre>

<p>Note: Filebeat’s configuration file is in YAML format, which means that indentation is very important! Be sure to use the same number of spaces that are indicated in these instructions.</p>

<p>Near the top of the file, you will see the prospectors section, which is where you can define prospectorsthat specify which log files should be shipped and how they should be handled. Each prospector is indicated by the - character.</p>

<p>We’ll modify the existing prospector to send syslog and auth.log to Logstash. Under paths, comment out the - <code>/var/log/*.log</code> file. This will prevent Filebeat from sending every .log in that directory to Logstash. Then add new entries for syslog and auth.log. It should look something like this when you’re done:</p>

<pre><code>...
      paths:
        - /var/log/auth.log
        - /var/log/syslog
#        - /var/log/*.log
...
</code></pre>

<p>Then find the line that specifies document_type:, uncomment it and change its value to “syslog”. It should look like this after the modification:</p>

<pre><code>...
      document_type: syslog
...
</code></pre>

<p>This specifies that the logs in this prospector are of type syslog (which is the type that our Logstash filter is looking for).</p>

<p>If you want to send other files to your ELK server, or make any changes to how Filebeat handles your logs, feel free to modify or add prospector entries.</p>

<p>Next, under the output section, find the line that says elasticsearch:, which indicates the Elasticsearch output section (which we are not going to use). Delete or comment out the entire Elasticsearch output section (up to the line that says #logstash:).</p>

<p>Find the commented out Logstash output section, indicated by the line that says #logstash:, and uncomment it by deleting the preceding #. In this section, uncomment the hosts: [&ldquo;localhost:5044&rdquo;]line. Change localhost to the private IP address (or hostname, if you went with that option) of your ELK server:</p>

<pre><code> ### Logstash as output
  logstash:
    # The Logstash hosts
    hosts: [&quot;ELK_server_private_IP:5044&quot;]
</code></pre>

<p>This configures Filebeat to connect to Logstash on your ELK Server at port 5044 (the port that we specified a Logstash input for earlier).</p>

<p>Directly under the hosts entry, and with the same indentation, add this line in filebeat.yml file:</p>

<pre><code>bulk_max_size: 1024
</code></pre>

<p>Next, find the tls section, and uncomment it. Then uncomment the line that specifies certificate_authorities, and change its value to [&ldquo;/etc/pki/tls/certs/logstash-forwarder.crt&rdquo;]. It should look something like this:</p>

<pre><code>...
    tls:
      # List of root certificates for HTTPS server verifications
      certificate_authorities: [&quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;]
</code></pre>

<p>This configures Filebeat to use the SSL certificate that we created on the ELK Server.</p>

<p>Save and quit.</p>

<p>Now restart Filebeat to put our changes into place:</p>

<pre><code>sudo service filebeat restart
sudo update-rc.d filebeat defaults 95 10
</code></pre>

<p>Again, if you’re not sure if your Filebeat configuration is correct, compare it against this <a href="https://gist.githubusercontent.com/thisismitch/3429023e8438cc25b86c/raw/de660ffdd3decacdcaf88109e5683e1eef75c01f/filebeat.yml-ubuntu">example Filebeat configuration</a>.</p>

<p>Now Filebeat is sending syslog and auth.log to Logstash on your ELK server! Repeat this section for all of the other servers that you wish to gather logs for.</p>

<h2 id="test-the-filebeat-installation">Test the filebeat installation:</h2>

<p>If your ELK stack is setup properly, Filebeat (on your client server) should be shipping your logs to Logstash on your ELK server. Logstash should be loading the Filebeat data into Elasticsearch in a date-stamped index, filebeat-YYYY.MM.DD.</p>

<p>On your ELK Server, verify that Elasticsearch is indeed receiving the data by querying for the Filebeat index with this command:</p>

<p><code>curl -XGET ‘http://localhost:9200/filebeat-*/_search?pretty&amp;#8217;</code></p>

<p>You should see a bunch of output that looks like this:</p>

<p>Sample Output:</p>

<pre><code>...
{
      &quot;_index&quot; : &quot;filebeat-2016.01.29&quot;,
      &quot;_type&quot; : &quot;log&quot;,
      &quot;_id&quot; : &quot;AVKO98yuaHvsHQLa53HE&quot;,
      &quot;_score&quot; : 1.0,
      &quot;_source&quot;:{&quot;message&quot;:&quot;Feb  3 14:34:00 rails sshd[963]: Server listening on :: port 22.&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2016-01-29T19:59:09.145Z&quot;,&quot;beat&quot;:{&quot;hostname&quot;:&quot;topbeat-u-03&quot;,&quot;name&quot;:&quot;topbeat-u-03&quot;},&quot;count&quot;:1,&quot;fields&quot;:null,&quot;input_type&quot;:&quot;log&quot;,&quot;offset&quot;:70,&quot;source&quot;:&quot;/var/log/auth.log&quot;,&quot;type&quot;:&quot;log&quot;,&quot;host&quot;:&quot;topbeat-u-03&quot;}
    }
...
</code></pre>

<p>If your output shows 0 total hits, Elasticsearch is not loading any logs under the index you searched for, and you should review your setup for errors. If you received the expected output, continue to the next step.</p>

<h2 id="connect-to-kibana">Connect to Kibana</h2>

<p>When you are finished setting up Filebeat on all of the servers that you want to gather logs for, let’s look at Kibana, the web interface that we installed earlier.</p>

<p>example:</p>

<p><a href="http://localhost:5601/">http://localhost:5601/</a>         and hit enter</p>

<p>In a web browser, go to the FQDN or public IP address of your ELK Server. After entering the “kibanaadmin” credentials, you should see a page prompting you to configure a default index pattern:</p>

<p><img src="https://blog.indonesiadot.com/images/2020/kibana1.png" alt="kibana1" /></p>

<p>Go ahead and select [filebeat]-YYY.MM.DD from the Index Patterns menu (left side), then click the Star (Set as default index) button to set the Filebeat index as the default.</p>

<p>Now click the Discover link in the top navigation bar. By default, this will show you all of the log data over the last 15 minutes. You should see a histogram with log events, with log messages below:</p>

<p><img src="iimages/2020/kibana2.png" alt="kibana2" /></p>

<p>Right now, there won’t be much in there because you are only gathering syslogs from your client servers. Here, you can search and browse through your logs. You can also customize your dashboard.</p>

<p>Try the following things:</p>

<p>Search for “root” to see if anyone is trying to log into your servers as root
Search for a particular hostname (search for host: &ldquo;hostname&rdquo;)
Change the time frame by selecting an area on the histogram or from the menu above
Click on messages below the histogram to see how the data is being filtered
Kibana has many other features, such as graphing and filtering, so feel free to poke around!</p>

<h2 id="conclusion">Conclusion</h2>

<p>Now that your syslogs are centralized via Elasticsearch and Logstash, and you are able to visualize them with Kibana, you should be off to a good start with centralizing all of your important logs. Remember that you can send pretty much any type of log or indexed data to Logstash, but the data becomes even more useful if it is parsed and structured with grok.</p>

<p>To improve your new ELK stack, you should look into gathering and filtering your other logs with Logstash, and <a href="https://www.digitalocean.com/community/tutorials/how-to-use-kibana-dashboards-and-visualizations">creating Kibana dashboards</a>. You may also want to <a href="https://www.digitalocean.com/community/tutorials/how-to-gather-infrastructure-metrics-with-topbeat-and-elk-on-ubuntu-14-04">gather system metrics by using Topbeat</a> with your ELK stack. All of these topics are covered in the other tutorials in this series.</p>

<p>Good luck!</p>

<p>Referensi:</p>

<ul>
<li><a href="https://mohitepramod.wordpress.com/2018/06/">https://mohitepramod.wordpress.com/2018/06/</a></li>
<li><a href="https://logz.io/blog/10-elasticsearch-concepts/">https://logz.io/blog/10-elasticsearch-concepts/</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_basic_concepts.html#_near_realtime_nrt">https://www.elastic.co/guide/en/elasticsearch/reference/current/_basic_concepts.html#_near_realtime_nrt</a></li>
<li><a href="https://qbox.io/blog/welcome-to-the-elk-stack-elasticsearch-logstash-kibana">https://qbox.io/blog/welcome-to-the-elk-stack-elasticsearch-logstash-kibana</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-centos-7">https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-centos-7</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04</a></li>
</ul>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/elk/">elk</a>, <a href="https://blog.indonesiadot.com/tags/logs/">logs</a>, <a href="https://blog.indonesiadot.com/tags/elasticsearch/">elasticsearch</a>, <a href="https://blog.indonesiadot.com/tags/logstash/">logstash</a>, <a href="https://blog.indonesiadot.com/tags/kibana/">kibana</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
    <div class="post">
        <h1 class="post-title"><a href="https://blog.indonesiadot.com/2020/03/18/ubuntu-tips/">Ubuntu tips</a></h1>
        <span class="post-date">Mar 18, 2020</span>
	

<h2 id="install-specific-version">Install specific version</h2>

<pre><code>apt-cache policy gparted
apt install gparted=0.16.1-1
</code></pre>

<ul>
<li><a href="https://askubuntu.com/questions/428772/how-to-install-specific-version-of-some-package">https://askubuntu.com/questions/428772/how-to-install-specific-version-of-some-package</a></li>
</ul>

<h2 id="apport-high-cpu">apport high cpu</h2>

<pre><code>killall -9 apport
service apport stop &amp;&amp; systemctl stop apport
rm -f /var/crash/*
apt-get remove apport -y
</code></pre>

	<hr/>
	
	
	 Tags:
	  <a href="https://blog.indonesiadot.com/tags/linux/">linux</a>, <a href="https://blog.indonesiadot.com/tags/ubuntu/">ubuntu</a>, <a href="https://blog.indonesiadot.com/tags/tips/">tips</a>
	 <hr/>
    </div>
  
</div>

<div class="pagination">
  
  <a class="pagination-item older" href="https://blog.indonesiadot.com/page/5/">Older</a>
  

  
  <a class="pagination-item newer" href="https://blog.indonesiadot.com/page/3/">Newer</a>
  
</div>

<hr/>


<hr/>
Made with &hearts; using <a href="https://https://gohugo.io">hugo</a> by willing of <a href="https://www.google.com/search?q=asmaul+husna">Al-Khooliq Al-Mushowwir Al-Baari'</a>
<p>&nbsp;</p>

	</div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
  </body>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-147159427-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-147159427-1');
</script>

</html>

